\documentclass[a4paper, 11pt, titlepage, onehalfspacing]{article}
%\usepackage{color}
%\definecolor{light-gray}{gray}{0.95}

\usepackage{xcolor}
\usepackage{alltt}
\usepackage{url}
\usepackage{tikz}
\usepackage{ulem}
\usepackage{setspace}
\usepackage{apacite}
\usepackage{wasysym}
\usetikzlibrary{trees}

\newcommand
{\image}[2]{\vspace{10 mm} \includegraphics[width=\textwidth]{#1}
\begin{center} \caption{#2} \end{center}
\vspace{10 mm}
}


\definecolor{light-gray}{gray}{0.95}
% Compensate for fbox sep:
\newcommand\Hi[2][light-gray]{%
  \hspace*{-\fboxsep}%
  \colorbox{#1}{#2}%
  \hspace*{-\fboxsep}%
}

% Command for inserting a todo item
%http://midtiby.blogspot.ie/2007/09/todo-notes-in-latex.html
\newcommand{\todo}[1]{%
% Add to todo list
\addcontentsline{tdo}{todo}{\protect{#1}}%
%
\begin{tikzpicture}[remember picture, baseline=-0.75ex]%
\node [coordinate] (inText) {};
\end{tikzpicture}%
%
% Make the margin par
\marginpar{%
\begin{tikzpicture}[remember picture]%
\definecolor{orange}{rgb}{1,0.5,0}
\draw node[draw=black, fill=orange, text width = 3cm] (inNote)
{#1};%
\end{tikzpicture}%
}%
%
\begin{tikzpicture}[remember picture, overlay]%
\draw[draw = orange, thick]
([yshift=-0.2cm] inText)
-| ([xshift=-0.2cm] inNote.west)
-| (inNote.west);
\end{tikzpicture}%
%
}%

\definecolor{light-gray}{gray}{0.95}
% Compensate for fbox sep:
\newcommand\hilight[2][light-gray]{%
  \hspace*{-\fboxsep}%
  \colorbox{#1}{#2}%
  \hspace*{-\fboxsep}%
}

\title{Browser-based Categorization of Data Towards Automated Visualization}
\author{Steven Diviney}

\begin{document}
\onehalfspacing
\maketitle

\newpage

\begin{abstract}
This paper presents a system to automatically generate suitable visualizations given arbitrary data. Data Visualization is an increasingly common technique used to reinforce human cognition. In many areas of human activity the volume of data being generated is increasing rapidly. New methods must be employed to assist in the comprehension of this data. There has been a good amount of research performed to assess what factors contribute to the creation of an effective visualization. Additionally many new and novel visualizations have been created. However, automatic generation of visualizations has received little attention.

Such a tool would help to combine these two areas of research. An understanding of what factors contribute to an effective visualization are encoded into the system presented. Given an arbitrary dataset the system attempts to select the most appropriate visualization. This paper also discusses to what extent this process is viable.

Using the limited amount of information contained in a raw dataset it is possible to select a comprehensible visualization. As the dataset becomes increasingly complex the effectiveness of such a system diminishes. A number of datasets are input to the system and an evaluation is undertaken. The results presented show that such a system can be used to assist users in the creation of suitable visualizations while avoiding the creation of inappropriate or even misleading visualizations.
\end{abstract}

\tableofcontents

\newpage
 
\section{Introduction} 
\todo{10/04/2013 changes in onenote}
This section introduces the dissertation topic and explains the motivation behind the work. A research question is purposed and a number of objectives are outlined in an attempt to satisfy it. This is followed by evaluation criteria and finally by a summary of the document structure.

\subsection{On Visualization}
There are three fields subfields of Data Visualization. The boundaries between them are not particularly distinct and they are referred to somewhat interchangeably in academic literature. There are many other types but these three are of particular interest as their primary concern is the visualization of large volumes of data almost always with the aid of a computer.
\subsubsection{Information Visualization}
Information visualization is perhaps the most broadly used and can be thought to encompass all of the fields of visualization. The term today is generally applied to the visual representation of large-scale collections of non-numerical information, such as text in a book or files on a hard-disk. The distinction between this definition and that of Data Visualization seems to be quite poor. The type of the data in question is used to distinguish the two. The terms ``information" and ``data" are not so easily distinguished. Information can be thought of as a level of abstraction above data. The mere fact that a dataset is non-numerical does not identify it as information. A set of ordinal labels or categories is just as meaningless as a set of numbers. Information is created by organizing such data and presenting it with context.

Information visualization is concerned with representing more abstract topics. A good example is process visualization. Each element in a process visualization represents a complex topic.

\subsubsection{Data Visualization}
Data Visualization is the science of visual representation of data, defined as ``facts and statistics collected together for reference or analysis" \cite{oed31}. As stated the distinction between data and Information Visualization is not very concrete. This distinction is generally not regarded as important but this paper is concerned with Data Visualization. The synthesis of new information through the creation of visual artifacts.

\subsubsection{Scientific Visualization}
Scientific Visualization is concerned primarily with the visualization of objects in three dimensional space with an emphasis on realistic rendering. This emphasis on realism is primarily what distinguishes it from other forms of visualization. This is not to say that the other forms may distort the data, rather that abstract data does not necessarily have a spatial dimension. How would one realistically visualize the lines of a book? Novel ways must be invented to accomplish this.

 
\subsection{Motivation and Description} 

Data Visualization is defined as an internal construction in the mind. The field of Data Visualization is connected with creating visual artifacts in order to facilitate individuals in building an internal representation of a dataset \cite{spence2001information}. Data Visualization is not a new field but it has only become an established area of scientific research in recent years \todo{Cite journal dates}. The volume of data a typical computer can generate and process far exceeds what a human can comprehend. Data Visualization is becoming an increasingly popular technique to aid this comprehension.

Information visualization is a growing area of research. Presently there is a good amount of discussion surrounding new and novel visualization techniques and how to create effective visualizations \todo{Cite IEEE Visualization Journal, Sage and others}. The articles presented in these journals typically focus on the creation of specific visualizations, new and interesting ways to graphically represent specific types of data. There is also a suitable body of literature concerning the  process of creating a visualization. There have been a number of books published on the subject \todo{Mazza, Shneiderman, Approche Graphique, Haskell etc}. These works outline the steps needed to visualize different types of data but do not attempt to automate the process.

The majority of new techniques exists in some degree of isolation. Individual implementations typically exist in near complete isolation, often with the dataset hard coded into the software. \todo{How do you back up a claim like this? Perhaps get rid of it}  There are a number of products \todo{Ref state of the art or just leave for SOA?} that attempt to create a complete end to end process for visualizing data sets. These products are either highly specialized \todo{again, state of the art, gretl would be a good example, need more} or lack complex visualizations and require user input throughout the process \todo{Excel}. 

Highly specialized applications such as gretl can afford to make numerous assumptions about the input dataset. It is assumed they will be used as part of a specific suite of tools and as such are able to directly process the proprietary output of such tools. Such output is often rich with meta-data which is used to assist the visualization process \todo{Either cover this in SOA or get rid of it}.

General purpose tools such as Microsoft Excel contain a number of simple charts and graphs. They require a basic level of user training to create and offer no assistance in selecting the most appropriate chart for a given dataset. This often leads to unsatisfactory, confusing or even misleading results \todo{Show a really bad example of excel output}. 

With a few exceptions, such as gretl, these tools are proprietary and lack documentation on the techniques they use \todo{This is another rather bold claim}.

This paper aims to address these deficiencies by providing a fully automated end to end visualization tool for arbitrary datasets. There have been some notable projects that accomplish such a goal. \todo{Polaris, Mackinlay. Need to look back over these and pick out what was expanded upon.} Emphasis has been placed on areas where previous works have relied on human actors to complete the process.

%-User Need
%-Gap in research
%	-Expose deficiencies of state of the art
%	-Outline what was taken from SOA, outline what was added.
%	-DO SOA first I suppose.


	\subsection{Research Question}

The objective of this project is to determine to what extent can suitable visualizations be dynamically and automatically generated using browser based technologies. The Automatic Classifier and Data Visualizer, or AC\lightning{}DV is presented. The goals for the project are as follows:
\begin{itemize}
\item Design and develop software capable of accepting arbitrary data in a specific format and display it using suitable visualizations. This should be done without an intervention from the user.
\item Access the level of benefit that the visualizations can bring to potential users from various fields.
\item Investigate to what extent visualizations can be generated given only an input dataset.
\item Elaborate on the potential of a more sophisticated version of the software using additional techniques to determine features of input datasets.
\end{itemize}
Browser based technologies have been selected due to their wide-scale support. The goal of the system is to simplify the process of visualization creation. Browser based technologies require no setup or configuration and thus simplify the process. \todo{But it's not an end user app so is this sufficient justification?}


	\subsection{Evaluation}

The system will be evaluated by inputting a number of datasets and assessing the output against the previously stated goals. A number of use cases have been drawn up to determine how beneficial such a tool is in aiding various users in visualizing data. \todo{Switching between tenses a bit here}. AC\lightning{}DV is not intended as an application suitable for end users so the user experience is not relevant to the evaluation. The goal of the system is to quickly produce effective visualizations that represent the data accurately.

Key to the evaluation is the notion of suitable visualizations. In order for a visualization to be considered useful it must meet several criteria. This will be outlined in detail in later sections \todo{Ref appropriate section}. Visualizations are composed of many individual elements with different attributes such as colour, size and spatial location. These elements have been the subject of previous research and a number of guidelines exists outlining their usage \todo{Cite Mazza, Jock etc. CALL them retinal variables}. However these guidelines are not hard rules and their exists no formal way to evaluate a complete visualization. Methods from the field of Human Computer Interaction are typically used, specifically user evaluations and trails. AC\lightning{}DV generates visualizations that are well understood, thus eliminating the need for lengthy user evaluations concerning the effectiveness of the individual output visualizations. The guidelines stated above are used by the system to pick appropriate visualizations. These guidelines will be used to benchmark the effectiveness of visualizations produced by the system.

	\subsection{Overview}
 
This chapter is followed by a survey of the State of the Art in the areas of automated visualization and the visualization process. Section 3 examines the design of the visualization tool and gives an overview of the process of Information Visualization. Section 4 gives a complete description of the projects implementation and any problems encountered. Section 5 is an evaluation of the project and a discussion of its merits and failures. The extent to which such a system is viable is also discussed in this section. The paper concludes with a look at potential future work that could extend AC\lightning{}DV.
%-Go into contents of each chapter a bit
%-Expose the argument thread. THIS is the focus. Stay with argument.


	\section{State Of The Art}

This section presents work currently being undertaken in two main areas of research discussed in this paper; automated visualization and the visualization process. An analysis of these projects conclude this section.

	\subsection{Visualization Process}


		\subsubsection{Introduction to Information Visualization}
Introduction to Information Visualization \cite{mazza2009introduction} is one of the most recent texts providing a state of the art in the field of information visualization. As well as listing many examples of newer more novel visualization techniques in the later chapters it also contains a substantial amount of information about the process of information visualization. This set of first principles has influenced the work presented in this paper quite heavily. The author acknowledges that given an arbitrary set of data there is no way to determine, automatically or otherwise, what visualization is a suitable representation of that data. There is in fact no agreed criteria to evaluate a finished visualization.

However there are certain characteristics of data that can be used to help select an appropriate type of visualization. These steps are outlined as follows:
\begin{itemize}
\item Define the problem: Is the goal of the visualization to communicate some meaning in the data, allow the user to explore the data or confirm a hypothesis.
\item Examine the nature of the data to represent: What type of data is being dealt with; ordinal, quantitative or categorical.
\item What is the dimensionality of the data: The number of attributes of the dataset. In a univariate dataset one attribute varies with respect to another. Bivariate and trivariate extend this with multivariate extending to four or more dimensions.
\item What is the structure of the data: This can be more easily understood by defining what types of data structures can be used to store the data; linear data structures such as vectors and tables, temporal data, spatial data hierarchies and networks.
\item What type of interaction will the user have with the data: can the user transform the data by modifying it, can the modify parameters used to display the data or is the visualization static.
\end{itemize}

This process is summarized in Table \ref{mazzatable} \cite{mazza2009introduction}. It will be revisited in later sections.

\begin{table}[!ht]
\centering
    \begin{tabular}{lllll}
    \hline
    Problem       & Data type    & Dimension    & Data Structures & Type of Interaction \\ \hline
    Communicative & Quantitative & Univariate   & Linear          & Static              \\
    Explore       & Ordinal      & Bivariate    & Temporal        & Transformable       \\
    Confirm       & Categorical  & Trivariate   & Spatial         & Manipulable         \\
    ~             & ~            & Multivariate & Hierarchical    & ~                   \\ 
    ~             & ~            & ~            & Network         & ~                   \\ \hline
    \end{tabular}
\caption{Variables to consider when designing visual representations.}
    \label{mazzatable}
\end{table}

This process eliminates visualizations that are not suitable for a given data set. Deciding on an individual visualization to use is a decidedly harder task. The effectiveness of a visualization is dependent on the perceptual capabilities of the viewer. As there is no empirically verified model of human perceptual abilities one is purposed. This model is based off the work of Jacques Bertin \cite{bertin1973semiologie}. Bertin was the first person to define a set of ``retinal variables"; how different properties of graphics can be used to convey various types of information. His work has been extended since it's original publication and the model presented here is perhaps one of the most up to date. A list of retinal variables is presented in order of effectiveness conveying different types information. This idea will be revisited throughout this paper and a complete model will be detailed in a later section. 

Mazza's work is a very complete introduction to information visualization and draws attention to all of the factors that need to be considered when creating a visualization. It is perhaps the most complete state of the art currently available and builds on a large amount of influential publications in the field. It has been very influential in the design of the system presented in this paper.


		\subsubsection{The Eyes have it}



	\subsection{Automated Visualization}

The focus of this paper is automated visualization, taking a dataset and generating a visualization from it without any user input. There are a few projects that incorporate some kind of automated data visualizer. Generally such a visualizer is part of a larger project and requires user interaction to function. This section presents two such projects with varying degrees of automation.

		\subsubsection{Polaris}
Polaris extends the well known pivot table interface \todo{citation} to display information visually. Multiple visualizations are displayed on a pivot table which the user can interact with by selecting or ``brushing" data-points to filter the displayed data. The visualizations act as interactive query builder that allow user to explore large datasets quite rapidly. First published in 2000 it has evolved into Tableau, a commercial visualizations product.

Polaris consists of two main components. A graphic generator and a database query generator. The database query generator will be omitted in this discussion as it is not relevant to AC\lightning{}DV. The graphic generator makes several assumptions about the nature of the input data. These assumptions are also made by AC\lightning{}DV\todo{Should this just go in design?}. Data is characterized as either quantitative or ordinal. Nominal data is assigned an ordering by the system and treated as ordinal data. Polaris assumes all nominal fields are dimensions, or independent variables, and all quantitative fields are measures, or dependent variables. Three different types of graphics can be generated based on the input data. User input is then used to refine this to a single generated graphic.

The user interacts with a relatively complex UI. The UI contains a number of ``shelves" onto which the user places data-sources and data records. The user also selects what type of mark to associate with each record. The selected mark and nature of the data is used by the system to determine what type of visualization to render. For example, if the data-source contains an ordinal and quantitative field and the user selects a bar as the mark to use a bar chart is generated. If the user selects a dot as the mark a dot plot is generated. An understanding of Bertins retinal variables \cite{bertin1973semiologie} is encoded into the system to ensure the visualizations are comprehensible. It is not clear how Polaris handles input that cannot be effectively displayed using these rules, e.g. when the input data set is too large to be displayed.

Polaris allows multiple series of data to be overlaid on the same visualization but it does not appear to allow more than two dimensions of data to be displayed at the same time. As it is a data exploration tool it requires user input too function. While it does contain an understanding of retinal variables these are used to automate the generation of mappings between data and graphical marks. It does not contain any rules governing how the various retinal variables are combined so it may be possible for a user to create ineffective or incomprehensible visualizations \todo{Go into a bit more and cite}.

		\subsubsection{A Presentation Tool}

A Presentation Tool, henceforth referred to as APT, is an application-independent presentation tool capable of automatically designing effective visualizations of relational information. It attempts to create a wide variety of designs and encode graphic design criteria in a useful form. It achieves this by defining graphical presentations as sentences in a graphical language. An expressiveness and effectiveness criteria are used to codify the graphic design criteria. The ``sentences" produced by these two systems are then combined using artificial intelligence techniques to create designs.

APT is divided into three parts, expressiveness, effectiveness and composition. Expressiveness determines how the input information can be expressed in a graphical language. It is encoded into a formal language. This languages main purpose is to ensure that any graphic encodes all of the input data and only the input data. It is not used to select entire graphics, rather it is used to test individual graphical mappings, i.e. to a single axis or type of mark.

Effectiveness is is used to determine which of the sentences generated by the expressiveness phase is most effective at displaying the data in a useful manner \todo{Double check expressiveness and effectiveness synchronous w.r.t each other.} It is dependent on the capabilities of the user. There is a difficulty in that no verified theory of human perceptual capabilities exists so one is purposed. It is based off Bertins retinal variables \cite{bertin1973semiologie} which have been ranked in order of effectiveness by Cleveland and McGill \cite{cleveland1984graphical}. This ranking has been extended to include rankings ordinal and nominal types of data although this extension has not been empirically verified \todo{Does Mazza match up with this ranking?}.

The system generates multi-dimensional graphics by combining the effectively one dimensional or single data series sentences generated in the expressiveness and effectiveness stages. Designs are merged on common data. The types of data to merge on are ordered in terms of effectiveness.
\begin{itemize}
\item Mark composition.
\item Double axis composition.
\item Single axis composition.
\end{itemize}
\todo{Replace the system with relevant name}
APT combines these three stages using artificial intelligence techniques. The algorithm has three steps: partition, selection and composition. Each step contains various choices. If these choices do not lead to design backtracking is used to consider others.
\begin{itemize}
\item Partitioning: Each set of relations, or columns in the database, are partitioned to match the criteria of of one of the sentences defined in the expressiveness stage.
\item Selection: A list of candidate designs for each partition is ordered in terms of effectiveness. 
\item Composition: The individual designs are composed into unified presentations of all the input data using the composition algebra.
\end{itemize}
\todo{Example in notes of bar-chart rule.}

Overall APT is a very robust and complex system. However it is perhaps somewhat dated. It is only capable of generating static graphics. It can also factor in the output media into the design choices, i.e. if the screen is monochrome colour is omitted for the effectiveness stage. \todo{More bad things because APT is pretty cool}

\todo{Published in 1986, perhaps too old?}
 

\section{Design}

	\subsection{Technologies used}

	\subsection{Overview of Data Visualization Process}

\todo{This will probably be broken down}


\todo{Section may be somewhat redundant due to section 2.2}

	\subsection{Architecture}

\todo{This will probably be broken down}


\section{Implementation}


\section{Evaluation and Discussion}


\section{Future Work and Conclusions}

\bibliographystyle{apacite}
\bibliography{bibliography}

\appendix
\section{Appendix A}

\end{document} 
